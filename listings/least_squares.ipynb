{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=6><b>GEOS 669 Geodetic Methods and Modeling</b></font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEAST SQUARES PARAMETER ESTIMATION Example\n",
    "\n",
    "The code below provides an example for simple least squares approximation in Python. We're giving ourselves a synthetic data set (line), add some noise, set up the matrix and vector components, perform the inversion and analyze the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward model of a line\n",
    "\n",
    "We will investigate the model for a line:\n",
    "\n",
    "$y = a+bx$\n",
    "\n",
    "by first giving ourselves some parameter values for $a$ and $b$, sampling this discretely and plotting the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward model of a line y = a+bx\n",
    "a       = 1\n",
    "b       = 3.5\n",
    "\n",
    "x = np.vstack(np.arange(0, 4, 0.1))b\n",
    "y = np.vstack(a+b*x)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y, 'o')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward model of a line with noise\n",
    "\n",
    "Now, let's add some noise, our model becomes:\n",
    "\n",
    "$y_n = a + bx + e$\n",
    "\n",
    "where $e$ is some noise, here following a normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.5\n",
    "\n",
    "noise = np.vstack(np.random.normal(0,std_dev,len(x)))\n",
    "yn = np.vstack(a+b*x+noise)\n",
    "\n",
    "print(\"Clean forward model (line) and noisy observations:\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, yn, \"o\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "\n",
    "# Now we can't see the line any more. These are our data.\n",
    "print(\"Clean forward model removed ... these are the measured data:\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, yn, \"o\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guessing a model\n",
    "\n",
    "Sometimes it seems appropriate to start out with a wild guess. After all, there's some kind of linear relationship in those data. Why not explore this. Key is always to assess the residuals. If there's some systematic trend in the residuals, your model (the parameter values you chose) isn't doing a good job explaining the observations. Occassionally, that's the goal - you want to isolate a signal that requires explanation using a different model. Still, you've gotta assess the residuals.\n",
    "\n",
    "We're giving ourselves a model guess, $a_{guess}$ and $b_{guess}$, to calculate synthetic observations $y_{guess}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_guess = 0.0\n",
    "b_guess = 4.5\n",
    "y_guess = np.vstack(a_guess + b_guess * x)\n",
    "\n",
    "print( \"Residuals after guessing a model (a=%f, b=%f) ...\" % (a_guess, b_guess))\n",
    "\n",
    "#G      = np.matrix(np.hstack((np.vstack(np.ones(len(x))), x)))\n",
    "#m      = np.vstack((a_guess,b_guess))\n",
    "resids = yn - y_guess\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(x, yn, \"o\")\n",
    "plt.plot(x, y_guess)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(x, resids, \"o\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"residuals\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Norms\n",
    "\n",
    "Once we have residuals in vector form, we can look at their norms. Goal of an optimization problem would be to minimize them, that is, find the parameter values which generate forward model values which are generally in good agreement with the data. Two are pretty common. The Euclidian or $L_2$ norm and the Absolute-value or $L_1$ norm:\n",
    "\n",
    "For residual vector $r$ with $i$ elements:\n",
    "\n",
    "$$ L_2 = (r_1^2 + r_2^2 + r_3^2 + r_4^2 \\dots)^{-1/2} = \\sqrt{\\sum_{i=1}^{N} r_i^2}$$\n",
    "$$ L_1 = |r_1| + |r_2| + |r_3| + |r_4| \\dots = \\sum_{i=1}^{N} |r_i| $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Respective norms ...\")\n",
    "\n",
    "print(\"L2-norm:  %f\" % ( math.sqrt(sum([v**2   for v in resids])) ))\n",
    "print(\"L1-norm:  %f\" % ( math.sqrt(sum([abs(v) for v in resids])) ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of the $L_1$ norm is robustness against outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multiply last residual by 100 to simulate large outlier... impact on norms:\")\n",
    "\n",
    "resids[-1] = resids[-1]*100\n",
    "print(\"L2-norm:  %f\" % ( math.sqrt(sum([v**2   for v in resids])) ))\n",
    "print(\"L1-norm:  %f\" % ( math.sqrt(sum([abs(v) for v in resids])) ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inversion\n",
    "\n",
    "Instead of messing around with trial and error through wild guesses, we can just move ahead and employ the normal equations which provide the least squares solution ($L_2$). The details of why this is are outside of the scope of this brief notebook. Checkout GEOS 627 to get the full background on that.\n",
    "\n",
    "So, the least squares solution, $m_{est}$, through the normal equations is:\n",
    "\n",
    "$$ m_{est} = (G^T G)^{-1} G^T d$$\n",
    "\n",
    "where, in our example, $d=y_{noise}=y_n$. We need to set up $G$, which needs to be set up such that the line equation above is adequately represented, meaning this will have two columns, the first constains $1$ and the other the elements of $x$ for equation $i$. There's some operations nexessary to make sure everything is of the correct orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the system matrix\n",
    "G = np.matrix(np.hstack( (np.vstack(np.ones(len(x))), x)) )\n",
    "\n",
    "#inversion\n",
    "m = (G.T*G).I * G.T * yn\n",
    "m = m.getA()\n",
    "\n",
    "print(\"Model parameter estimates from normal equation solution:\")\n",
    "print(\"a_est = %.2f (a = %.2f)\\nb_est = %.2f (b = %.2f)\" % (m[0], a, m[1], b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare model to data\n",
    "\n",
    "As a last step, you calculate the forward model $yf$ using the just estimated parameters in $m_{est}$. We can plot the model directly over the data, but also have to look at the residuals, which now are much more scattered looking and don't show much / any substantial systematic trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Forward model from estimated model parameters and noisy data ...\")\n",
    "\n",
    "#calculate forward model\n",
    "yf = m[0]+m[1]*x\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, yn, \"o\")\n",
    "plt.plot(x, yf)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "\n",
    "#calculate residuals and show norms:\n",
    "resids = yn-yf\n",
    "print(\"Residual norms:\")\n",
    "print(\"L2-norm:  %f\" % ( math.sqrt(sum([v**2   for v in resids])) ))\n",
    "print(\"L1-norm:  %f\" % ( math.sqrt(sum([abs(v) for v in resids])) ))\n",
    "\n",
    "print(\"\\n\\nResiduals ...\")\n",
    "plt.figure()\n",
    "plt.plot(x, resids, \"o\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"residuals\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
